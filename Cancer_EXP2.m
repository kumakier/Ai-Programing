% Solve a Pattern Recognition Problem with a Neural Network
% Script generated by Neural Pattern Recognition app
% Created 14-Mar-2019 14:50:09
%
% This script assumes these variables are defined:
%
%   cancerInputs - input data.
%   cancerTargets - target data.
close all
clear all
load cancer_dataset.mat
x = cancerInputs;
t = cancerTargets;

% Choose a Training Function
% For a list of all training functions type: help nntrain
% 'trainlm' is usually fastest.
% 'trainbr' takes longer but may be better for challenging problems.
% 'trainscg' uses less memory. Suitable in low memory situations.
trainFcn = 'trainscg';  % Scaled conjugate gradient backpropagation.

epochs = [3 5 7] %8 16 32 64];

nodes = [2 8 10]%8 32];

N = 30;

loopcountFor30 = 0
Train_result = zeros(length(epochs),length(nodes),2);
Test_result = zeros(length(epochs),length(nodes),2);
% Create a Pattern Recognition Network
for N = 1:30
    loopcountFor30 = loopcountFor30 +1;
    %randmise rows
    combineXY = [x;t];
    random_combineXT = combineXY(:,randperm(size(combineXY,1)))
    CancerInputs_rand = random_combineXT(1:9,:)
    Cancertargets_rand = random_combineXT(10:11,:)
    
    
    likelihood = zeros(6,669)
    error_classifiers = zeros(1,3)
    
%     net = setwb(net,rand(10,1));
%     net.IW{1,1}
%     net.b{1}
    
%     for n_epoch = 1:length(epoch)
%         for m_nodes = 1:length(nodes)
    ErrorTrainSets = zeros(1,3)
        ErrorTestSets = zeros(1,3)
loopcountForClassifier = 0        
    for j = 1:3
       loopcountForClassifier = loopcountForClassifier + 1;
        hiddenLayerSize = nodes(j);
        net = patternnet(hiddenLayerSize, trainFcn);            
        net.trainParam.epochs = epochs(j);

        
        % Choose Input and Output Pre/Post-Processing Functions
        % For a list of all processing functions type: help nnprocess
        net.input.processFcns = {'removeconstantrows','mapminmax'};

        

        % Setup Division of Data for Training, Validation, Testing
        % For a list of all data division functions type: help nndivision
        net.divideFcn = 'divideblock';  % Divide data randomly
        net.divideMode = 'sample';  % Divide up every sample
        
        net.divideParam.trainRatio = 50/100;
        net.divideParam.valRatio = 1/100;
        net.divideParam.testRatio = 49/100;

        % Choose a Performance Function
        % For a list of all performance functions type: help nnperformance
        net.performFcn = 'crossentropy';  % Cross-Entropy

        % Choose Plot Functions
        % For a list of all plot functions type: help nnplot
        net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
            'plotconfusion', 'plotroc'};
        
        
        
        
       
            % Train the Network
            [net,tr] = train(net,x,t);

            % Test the Network
            y = net(x);
            likelihood = y;
            
            e = gsubtract(t,y);
            error_classifiers = e;
            
            
            
            % modify this line change to test index
            
            
            % Recalculate Training, Validation and Test Performance
            %train
            trainTargets = t .* tr.trainMask{1};
%             trainTargets = trainTargets(~isnan(trainTargets))';
%             trainTargets = reshape(trainTargets, [2,length(trainTargets)/2]);
            testTargets = t .* tr.testMask{1};
%             testTargets = testTargets(~isnan(testTargets))';
%             testTargets = reshape(testTargets, [2,length(testTargets)/2]);
            
            TrainSetError = confusion(trainTargets,y)
            TestSetError = confusion(testTargets,y)
            
             
            ErrorTrainSets(j) = TrainSetError
            ErrorTestSets(j) = TestSetError
            
            ErrorTrainMean = mean(ErrorTrainSets);
            ErrorTestMean = mean(ErrorTrainSets);
%             trainOutput = y.* tr.trainMask{1};
%             trainOutput = trainOutput(~isnan(trainOutput))';
%             trainOutput = reshape(trainOutput, [2,length(trainOutput)/2]);
%             trainTargetind = vec2ind(trainTargets);
%             trainOutputind = m(trainOutput);
%             percentErrors(c,2) = sum(trainTargetind ~= trainOutputind)/numel(trainTargetind);
%             
%             %Test
%            
%             testOutput = y.* tr.testMask{1};
%             testOutput = testOutput(~isnan(testOutput))';
%             testOutput = reshape(testOutput, [2,length(testOutput)/2]);
%             testTargetind = vec2ind(testTargets);
%             testOutputind = vec2ind(testOutput);
%             percentErrors(c,3) = sum(testTargetind ~= testOutputind)/numel(testTargetind);
                
            %Val
            %valTargets = t .* tr.valMask{1};
            
            %Perfermances for train and test and val
            trainPerformance = perform(net,trainTargets,y);
%             valPerformance = perform(net,valTargets,y);
            testPerformance = perform(net,testTargets,y);

    end
      
        % View the Network
        % view(net)

%         Error_Avg = mean(percentErrors(:,3))
%         Error_Std = std(percentErrors(:,3))
%         Test_result(n_epoch , m_nodes , :) = [Error_Avg, Error_Std];
      
        end

%Now Plot the results

% figure();
% hold on;
% %mesh(nodes,epochs,result(:,:,1))
% subplot(2,2,1);
% for e_count = 1:length(epoch)
%     plot(1:length(nodes),Test_result(e_count,:,1),"-o")
%     hold on;
% end
% title("nodes vs error rate");
% xlabel("number of nodes");
% ylabel("error rate");
% legend
% %legend("epochs");
% 
% subplot(2,2,2);
% for m_count = 1:length(nodes)
%     plot(1:length(epoch),Test_result(:,m_count,1),"-o")
%     hold on;
% end
% title("epochs vs error rate");
% xlabel("number of epochs");
% ylabel("error rate");
% legend
% 
% subplot(2,2,3);
% for e_count = 1:length(epoch)
%     plot(1:length(nodes),Test_result(e_count,:,2),"-o")
%     hold on;
% end
% title("nodes vs std");
% xlabel("number of nodes");
% ylabel("std");
% legend
% %legend("epochs");
% 
% subplot(2,2,4);
% for n_count = 1:length(nodes)
%     plot(1:length(epoch),Test_result(:,n_count,2),"-o")
%     hold on;
% end
% title("epochs vs std");
% xlabel("number of epochs");
% ylabel("std");
% legend
% %legend("nodes");
% 
% 
% 
% %Find the minium test error
% min_Avg = min(min(Test_result(:,:,1)));
% 
% %%Find the minium Epoch and Node to find the optimal value
% 
% [min_Epoch, min_Node]=find(Test_result==min_Avg);
% min_Node = nodes(min_Node);
% min_Epoch = epoch(min_Epoch);

% Plots
% Uncomment these lines to enable various plots.
%figure, plotperform(tr)
%figure, plottrainstate(tr)
%figure, ploterrhist(e)
%figure, plotconfusion(t,y)
%figure, plotroc(t,y)

% Deployment
% Change the (false) values to (true) to enable the following code blocks.
% See the help for each generation function for more information.
% if (false)
%     % Generate MATLAB function for neural network for application
%     % deployment in MATLAB scripts or with MATLAB Compiler and Builder
%     % tools, or simply to examine the calculations your trained neural
%     % network performs.
%     genFunction(net,'myNeuralNetworkFunction');
%     y = myNeuralNetworkFunction(x);
% end
% if (false)
%     % Generate a matrix-only MATLAB function for neural network code
%     % generation with MATLAB Coder tools.
%     genFunction(net,'myNeuralNetworkFunction','MatrixOnly','yes');
%     y = myNeuralNetworkFunction(x);
% end
% if (false)
%     % Generate a Simulink diagram for simulation or deployment with.
%     % Simulink Coder tools.
%     gensim(net);
% end
